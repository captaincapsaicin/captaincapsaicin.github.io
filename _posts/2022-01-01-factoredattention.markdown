---
layout: post
title:  "Interpreting Potts and Transformer Protein Models Through the Lens of Simplified Attention"
date:   2022-01-21 22:21:59 +00:00
image: images/fatt.png
categories: research
author: "Neil Thomas"
authors: "Nicholas Bhattacharya*, <u>Neil Thomas</u>*, Roshan Rao, Justas Dauparas, Peter K. Koo, David Baker, Yun S. Song, Sergey Ovchinnikov"
venue: "Pacific Symposium on Biocomputing"
paper: https://psb.stanford.edu/psb-online/proceedings/psb22/bhattacharya.pdf
code: https://github.com/songlab-cal/factored-attention
twitter: https://twitter.com/countablyfinite/status/1347302237182152705
talk: https://www.loom.com/share/a08b71c17a1145fcbd9a3946b5d20e9a
---
Introduces "factored attention," a simplified attention layer that, in a certain limit, recovers a Potts model.